---
abstract: We consider learning methods based on the regularization of a convex empirical
  risk by a squared Hilbertian norm, a setting that includes linear predictors and
  non-linear predictors through positive-definite kernels. In order to go beyond the
  generic analysis leading to convergence rates of the excess risk as $O(1/\sqrt{n})$
  from $n$ observations, we assume that the individual losses are self-concordant,
  that is, their third-order derivatives are bounded by their second-order derivatives.
  This setting includes least-squares, as well as all generalized linear models such
  as logistic and softmax regression. For this class of losses, we provide a bias-variance
  decomposition and show that the assumptions commonly made in least-squares regression,
  such as the source and capacity conditions, can be adapted to obtain fast non-asymptotic
  rates of convergence by improving the bias terms, the variance terms or both.
section: contributed
title: 'Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization
  through Self-Concordance'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: marteau-ferey19a
month: 0
tex_title: 'Beyond Least-Squares: Fast Rates for Regularized Empirical Risk Minimization
  through Self-Concordance'
firstpage: 2294
lastpage: 2340
page: 2294-2340
order: 2294
cycles: false
bibtex_author: Marteau{-}Ferey, Ulysse and Ostrovskii, Dmitrii and Bach, Francis and
  Rudi, Alessandro
author:
- given: Ulysse
  family: Marteau-Ferey
- given: Dmitrii
  family: Ostrovskii
- given: Francis
  family: Bach
- given: Alessandro
  family: Rudi
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/marteau-ferey19a/marteau-ferey19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
