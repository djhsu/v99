---
abstract: 'We introduce and study a class of probabilistic generative models, where
  the latent object is a finite-dimensional diffusion process on a finite time interval
  and the observed variable is drawn conditionally on the terminal point of the diffusion.
  We make the following contributions: We provide a unified viewpoint on both sampling
  and variational inference in such generative models through the lens of stochastic
  control. We quantify the expressiveness of diffusion-based generative models. Specifically,
  we show that one can efficiently sample from a wide class of terminal target distributions
  by choosing the drift of the latent diffusion from the class of multilayer feedforward
  neural nets, with the accuracy of sampling measured by the Kullbackâ€“Leibler divergence
  to the target distribution. Finally, we present and analyze a scheme for unbiased
  simulation of generative models with latent diffusions and provide bounds on the
  variance of the resulting estimators. This scheme can be implemented as a deep generative
  model with a random number of layers. '
section: contributed
title: Theoretical guarantees for sampling and inference in generative models with
  latent diffusions
layout: inproceedings
series: Proceedings of Machine Learning Research
id: tzen19a
month: 0
tex_title: Theoretical guarantees for sampling and inference in generative models
  with latent diffusions
firstpage: 3084
lastpage: 3114
page: 3084-3114
order: 3084
cycles: false
bibtex_author: Tzen, Belinda and Raginsky, Maxim
author:
- given: Belinda
  family: Tzen
- given: Maxim
  family: Raginsky
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/tzen19a/tzen19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
