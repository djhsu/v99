---
abstract: Variance reduction techniques like SVRG provide simple and fast algorithms
  for optimizing a convex finite-sum objective. For nonconvex objectives, these techniques
  can also find a first-order stationary point (with small gradient). However, in
  nonconvex optimization it is often crucial to find a second-order stationary point
  (with small gradient and almost PSD hessian). In this paper, we show that Stabilized
  SVRG (a simple variant of SVRG) can find an $\epsilon$-second-order stationary point
  using only $\widetilde{O}(n^{2/3}/\epsilon^2+n/\epsilon^{1.5})$ stochastic gradients.
  To our best knowledge, this is the first second-order guarantee for a simple variant
  of SVRG. The running time almost matches the known guarantees for finding $\epsilon$-first-order
  stationary points.
section: contributed
title: 'Stabilized SVRG: Simple Variance Reduction for Nonconvex Optimization'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ge19a
month: 0
tex_title: 'Stabilized SVRG: Simple Variance Reduction for Nonconvex Optimization'
firstpage: 1394
lastpage: 1448
page: 1394-1448
order: 1394
cycles: false
bibtex_author: Ge, Rong and Li, Zhize and Wang, Weiyao and Wang, Xiang
author:
- given: Rong
  family: Ge
- given: Zhize
  family: Li
- given: Weiyao
  family: Wang
- given: Xiang
  family: Wang
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/ge19a/ge19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
