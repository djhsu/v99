---
abstract: We study the dynamics of gradient descent on objective  functions of the
  form $f(\prod_{i=1}^{k} w_i)$ (with respect to scalar  parameters $w_1,\ldots,w_k$),
  which arise in the context of  training depth-$k$ linear neural networks. We prove
  that for standard  random initializations, and under mild assumptions on $f$, the
  number of  iterations required for convergence scales exponentially with the depth  $k$.
  We also show empirically that this phenomenon can occur in higher  dimensions, where
  each $w_i$ is a matrix. This highlights a potential  obstacle in understanding the
  convergence of gradient-based methods for  deep linear neural networks, where $k$
  is large.
section: contributed
title: Exponential Convergence Time of Gradient Descent for    One-Dimensional Deep
  Linear Neural Networks
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shamir19a
month: 0
tex_title: Exponential Convergence Time of Gradient Descent for    One-Dimensional
  Deep Linear Neural Networks
firstpage: 2691
lastpage: 2713
page: 2691-2713
order: 2691
cycles: false
bibtex_author: Shamir, Ohad
author:
- given: Ohad
  family: Shamir
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/shamir19a/shamir19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
