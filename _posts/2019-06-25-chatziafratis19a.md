---
abstract: We prove that the evolution of weight vectors in online gradient descent
  can encode arbitrary polynomial-space computations, even in very simple learning
  settings. Our results imply that, under weak complexity-theoretic assumptions, it
  is impossible to reason efficiently about the fine-grained behavior of online gradient
  descent.
section: contributed
title: On the Computational Power of Online Gradient Descent
layout: inproceedings
series: Proceedings of Machine Learning Research
id: chatziafratis19a
month: 0
tex_title: On the Computational Power of Online Gradient Descent
firstpage: 624
lastpage: 662
page: 624-662
order: 624
cycles: false
bibtex_author: Chatziafratis, Vaggos and Roughgarden, Tim and Wang, Joshua R.
author:
- given: Vaggos
  family: Chatziafratis
- given: Tim
  family: Roughgarden
- given: Joshua R.
  family: Wang
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/chatziafratis19a/chatziafratis19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
