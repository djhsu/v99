---
abstract: This paper defines the notion of class discrepancy for families of functions.
  It shows that low discrepancy classes admit small offline and streaming coresets.
  We provide general techniques for bounding the class discrepancy of machine learning
  problems. As corollaries of the general technique we bound the discrepancy of logistic
  regression, sigmoid activation loss, matrix covariance, kernel density and any analytic
  function of the dot product or the squared distance. Our result resolves a long-standing
  open problem regarding the coreset complexity of Gaussian kernel density estimation.
  We provide two more related but independent results. First, an exponential improvement
  of the widely used merge-and-reduce trick which gives improved streaming sketches
  for any low discrepancy problem. Second, an extremely simple deterministic algorithm
  for finding low discrepancy sequences (and therefore coresets) for any positive
  semi-definite kernel. This paper establishes some explicit connections between class
  discrepancy, coreset complexity, learnability, and streaming algorithms.
section: contributed
title: Discrepancy, Coresets, and Sketches in Machine Learning
layout: inproceedings
series: Proceedings of Machine Learning Research
id: karnin19a
month: 0
tex_title: Discrepancy, Coresets, and Sketches in Machine Learning
firstpage: 1975
lastpage: 1993
page: 1975-1993
order: 1975
cycles: false
bibtex_author: Karnin, Zohar and Liberty, Edo
author:
- given: Zohar
  family: Karnin
- given: Edo
  family: Liberty
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/karnin19a/karnin19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
