---
abstract: 'Folklore results in the theory of Stochastic Approximation indicates the
  (minimax) optimality of Stochastic Gradient Descent (SGD) (Robbins and Monro, 1951)
  with polynomially decaying stepsizes and iterate averaging (Ruppert, 1988; Polyak
  and Juditsky, 1992) for classes of stochastic convex optimization. Basing of these
  folkore results and some recent developments, this manuscript considers a more subtle
  question: does any algorithm necessarily (information theoretically) have to query
  iterates that are sub-optimal infinitely often?'
section: open
title: 'Open Problem: Do Good Algorithms Necessarily Query Bad Points?'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ge19b
month: 0
tex_title: 'Open Problem: Do Good Algorithms Necessarily Query Bad Points?'
firstpage: 3190
lastpage: 3193
page: 3190-3193
order: 3190
cycles: false
bibtex_author: Ge, Rong and Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and
  Nagaraj, Dheeraj M. and Netrapalli, Praneeth
author:
- given: Rong
  family: Ge
- given: Prateek
  family: Jain
- given: Sham M.
  family: Kakade
- given: Rahul
  family: Kidambi
- given: Dheeraj M.
  family: Nagaraj
- given: Praneeth
  family: Netrapalli
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/ge19b/ge19b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
