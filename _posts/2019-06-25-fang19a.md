---
abstract: In this paper, we give a sharp analysis for  Stochastic Gradient Descent
  (SGD)  and  prove that SGD is able to efficiently escape from  saddle points and
  find an $(\epsilon, O(\epsilon^{0.5}))$-approximate second-order stationary point
  in $\tilde{O}(\epsilon^{-3.5})$ stochastic gradient computations for generic nonconvex
  optimization problems, when the objective function satisfies  gradient-Lipschitz,
  Hessian-Lipschitz, and dispersive noise assumptions. This  result subverts the classical
  belief that  SGD requires at least $O(\epsilon^{-4})$ stochastic gradient computations
  for obtaining an $(\epsilon,O(\epsilon^{0.5}))$-approximate second-order stationary
  point. Such SGD rate matches, up to a polylogarithmic factor of problem-dependent
  parameters, the rate of most accelerated  nonconvex stochastic optimization algorithms
  that adopt additional techniques, such as Nesterovâ€™s momentum acceleration, negative
  curvature search, as well as quadratic and cubic regularization tricks. Our novel
  analysis gives new insights into nonconvex SGD and can be potentially generalized
  to a broad class of stochastic optimization algorithms.
section: contributed
title: Sharp Analysis for Nonconvex SGD Escaping from Saddle Points
layout: inproceedings
series: Proceedings of Machine Learning Research
id: fang19a
month: 0
tex_title: Sharp Analysis for Nonconvex SGD Escaping from Saddle Points
firstpage: 1192
lastpage: 1234
page: 1192-1234
order: 1192
cycles: false
bibtex_author: Fang, Cong and Lin, Zhouchen and Zhang, Tong
author:
- given: Cong
  family: Fang
- given: Zhouchen
  family: Lin
- given: Tong
  family: Zhang
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/fang19a/fang19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
