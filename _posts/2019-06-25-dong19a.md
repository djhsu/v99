---
abstract: We study the logistic bandit, in which rewards are binary with success probability
  $\exp(\beta a^\top \theta) / (1 + \exp(\beta a^\top \theta))$ and actions $a$ and
  coefficients $\theta$ are within the $d$-dimensional unit ball.  While prior regret
  bounds for algorithms that address the logistic bandit exhibit exponential dependence
  on the slope parameter $\beta$, we establish a regret bound for Thompson sampling
  that is independent of $\beta$.  Specifically, we establish that, when the set of
  feasible actions is identical to the set of possible coefficient vectors, the Bayesian
  regret of Thompson sampling is $\tilde{O}(d\sqrt{T})$.  We also establish a $\tilde{O}(\sqrt{d\eta
  T}/\Delta)$ bound that applies more broadly, where $\Delta$ is the worst-case optimal
  log-odds and $\eta$ is the “fragility dimension,” a new statistic we define to capture
  the degree to which an optimal action for one model fails to satisfice for others.  We
  demonstrate that the fragility dimension plays an essential role by showing that,
  for any $\epsilon > 0$, no algorithm can achieve $\mathrm{poly}(d, 1/\Delta)\cdot
  T^{1-\epsilon}$ regret.
section: contributed
title: On the Performance of Thompson Sampling on Logistic Bandits
layout: inproceedings
series: Proceedings of Machine Learning Research
id: dong19a
month: 0
tex_title: On the Performance of Thompson Sampling on Logistic Bandits
firstpage: 1158
lastpage: 1160
page: 1158-1160
order: 1158
cycles: false
bibtex_author: Dong, Shi and Ma, Tengyu and Van Roy, Benjamin
author:
- given: Shi
  family: Dong
- given: Tengyu
  family: Ma
- given: Benjamin
  family: Van Roy
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/dong19a/dong19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
