---
abstract: Stochastic approximation (SA) is a key method used in statistical learning.
  Recently, its non-asymptotic convergence analysis has been considered in many papers.
  However, most of the prior analyses are made under restrictive assumptions such
  as unbiased gradient estimates and convex objective function, which significantly
  limit their applications to sophisticated tasks such as online and reinforcement
  learning. These restrictions are all essentially relaxed in this work. In particular,
  we analyze a general SA scheme to minimize a non-convex, smooth objective function.
  We consider update procedure whose drift term depends on a state-dependent Markov
  chain and the mean field is not necessarily of gradient type, covering approximate
  second-order method and allowing asymptotic bias for the one-step updates. We illustrate
  these settings with the online EM algorithm and the policy-gradient method for average
  reward maximization in reinforcement learning.
section: contributed
title: Non-asymptotic Analysis of Biased Stochastic Approximation Scheme
layout: inproceedings
series: Proceedings of Machine Learning Research
id: karimi19a
month: 0
tex_title: Non-asymptotic Analysis of Biased Stochastic Approximation Scheme
firstpage: 1944
lastpage: 1974
page: 1944-1974
order: 1944
cycles: false
bibtex_author: Karimi, Belhal and Miasojedow, Blazej and Moulines, Eric and Wai, Hoi-To
author:
- given: Belhal
  family: Karimi
- given: Blazej
  family: Miasojedow
- given: Eric
  family: Moulines
- given: Hoi-To
  family: Wai
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/karimi19a/karimi19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
