---
abstract: 'We study the complexity of training neural network models with one hidden
  nonlinear activation layer and an output weighted sum layer. We analyze Gradient
  Descent applied to learning a bounded target function on $n$ real-valued inputs.  %by
  training a neural network with a single hidden layer of nonlinear gates. We give
  an agnostic learning guarantee for GD: starting from a randomly initialized network,
  it converges in mean squared loss to the minimum error (in $2$-norm) of the best
  approximation of the target function using a polynomial of degree at most $k$. Moreover,
  for any $k$, the size of the network and number of iterations needed are both bounded
  by $n^{O(k)}\log(1/\epsilon)$. The core of our analysis is the following existence
  theorem, which is of independent interest:  for any $\epsilon > 0$, any bounded
  function that has a degree $k$ polynomial approximation with error $\epsilon_0$
  (in $2$-norm), can be approximated to within error $\epsilon_0 + \epsilon$ as a
  linear combination of $n^{O(k)}\cdot \mbox{poly}(1/\epsilon)$ {\em randomly chosen}
  gates from any class of gates whose corresponding activation function has nonzero
  coefficients in its harmonic expansion for degrees up to $k$. In particular, this
  applies to training networks  of unbiased sigmoids and ReLUs.  We also rigorously
  explain the empirical finding that gradient descent discovers lower frequency Fourier
  components before higher frequency components.  We complement this result with nearly
  matching lower bounds in the Statistical Query model. GD fits well in the SQ framework
  since each training step is determined by an expectation over the input distribution.
  We show that any SQ algorithm that achieves significant improvement over a constant
  function with queries of tolerance some inverse polynomial in the input dimensionality
  $n$ must use $n^{\Omega(k)}$ queries even when the target functions are restricted
  to a set of $n^{O(k)}$ degree-$k$ polynomials, and the input distribution is uniform
  over the unit sphere; for this class the information-theoretic lower bound is only
  $\Theta(k \log n)$.  Our approach for both parts is based on spherical harmonics.
  We view gradient descent as an operator on the space of functions, and study its
  dynamics. An essential tool is the Funk-Hecke theorem, which explains the eigenfunctions
  of this operator in the case of the mean squared loss.'
section: contributed
title: 'Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence
  and SQ Lower Bounds'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: vempala19a
month: 0
tex_title: 'Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence
  and SQ Lower Bounds'
firstpage: 3115
lastpage: 3117
page: 3115-3117
order: 3115
cycles: false
bibtex_author: Vempala, Santosh and Wilmes, John
author:
- given: Santosh
  family: Vempala
- given: John
  family: Wilmes
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/vempala19a/vempala19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
