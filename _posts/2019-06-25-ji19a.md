---
abstract: Gradient descent, when applied to the task of logistic regression, outputs
  iterates which are biased to follow a unique ray defined by the data. The direction
  of this ray is the maximum margin predictor of a maximal linearly separable subset
  of the data; the gradient descent iterates converge to this ray in direction at
  the rate $\cO(\nicefrac{\ln\ln t }{\ln t})$. The ray does not pass through the origin
  in general, and its offset is the bounded global optimum of the risk over the remaining
  data; gradient descent recovers this offset at a rate $\cO(\nicefrac{(\ln t)^2}{\sqrt{t}})$.
section: contributed
title: The implicit bias of gradient descent on nonseparable data
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ji19a
month: 0
tex_title: The implicit bias of gradient descent on nonseparable data
firstpage: 1772
lastpage: 1798
page: 1772-1798
order: 1772
cycles: false
bibtex_author: Ji, Ziwei and Telgarsky, Matus
author:
- given: Ziwei
  family: Ji
- given: Matus
  family: Telgarsky
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/ji19a/ji19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
