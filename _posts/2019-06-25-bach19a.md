---
abstract: We consider variational inequalities coming from monotone operators, a setting
  that includes convex minimization and convex-concave saddle-point problems.  We
  assume an access to potentially noisy unbiased values of the monotone operators
  and assess convergence through a compatible gap function which corresponds to the
  standard optimality criteria in the aforementioned subcases. We present a  universal
  algorithm for these inequalities based on the Mirror-Prox algorithm. Concretely,
  our algorithm \emph{simultaneously} achieves the optimal rates for the smooth/non-smooth,
  and noisy/noiseless settings. This is done without any prior knowledge of these
  properties, and in the  general set-up of arbitrary norms and compatible Bregman
  divergences. For convex minimization and convex-concave saddle-point problems, this
  leads to new adaptive algorithms. Our method relies on a novel yet simple adaptive
  choice of the step-size, which can be seen as  the appropriate  extension of AdaGrad
  to  handle constrained problems.
section: contributed
title: A Universal Algorithm for Variational Inequalities Adaptive to Smoothness and
  Noise
layout: inproceedings
series: Proceedings of Machine Learning Research
id: bach19a
month: 0
tex_title: A Universal Algorithm for Variational Inequalities Adaptive to Smoothness
  and Noise
firstpage: 164
lastpage: 194
page: 164-194
order: 164
cycles: false
bibtex_author: Bach, Francis and Levy, Kfir Y
author:
- given: Francis
  family: Bach
- given: Kfir Y
  family: Levy
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/bach19a/bach19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
