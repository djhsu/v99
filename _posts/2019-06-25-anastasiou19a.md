---
abstract: We provide non-asymptotic convergence rates of the Polyak-Ruppert averaged
  stochastic gradient descent (SGD) to a normal random vector for a class of twice-differentiable
  test functions. A crucial intermediate step is proving a non-asymptotic martingale
  central limit theorem (CLT), i.e., establishing the rates of convergence of a multivariate
  martingale difference sequence to a normal random vector, which might be of independent
  interest. We obtain the explicit rates for the multivariate martingale CLT using
  a combination of Stein?s method and Lindeberg?s argument, which is then used in
  conjunction with a non-asymptotic analysis of averaged SGD proposed in [PJ92]. Our
  results have potentially interesting consequences for computing confidence intervals
  for parameter estimation with SGD and constructing hypothesis tests with SGD that
  are valid in a non-asymptotic sense
section: contributed
title: Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic Rates
  of Martingale CLT
layout: inproceedings
series: Proceedings of Machine Learning Research
id: anastasiou19a
month: 0
tex_title: Normal Approximation for Stochastic Gradient Descent via Non-Asymptotic
  Rates of Martingale CLT
firstpage: 115
lastpage: 137
page: 115-137
order: 115
cycles: false
bibtex_author: Anastasiou, Andreas and Balasubramanian, Krishnakumar and Erdogdu,
  Murat A.
author:
- given: Andreas
  family: Anastasiou
- given: Krishnakumar
  family: Balasubramanian
- given: Murat A.
  family: Erdogdu
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/anastasiou19a/anastasiou19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
