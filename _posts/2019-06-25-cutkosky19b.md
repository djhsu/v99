---
abstract: 'We show how to take any two parameter-free online learning algorithms with
  different regret guarantees and obtain a single algorithm whose regret is the minimum
  of the two base algorithms. Our method is embarrassingly simple: just add the iterates.
  This trick can generate efficient algorithms that adapt to many norms simultaneously,
  as well as providing diagonal-style algorithms that still maintain dimension-free
  guarantees. We then proceed to show how a variant on this idea yields a black-box
  procedure for generating \emph{optimistic} online learning algorithms. This yields
  the first optimistic regret guarantees in the unconstrained setting and generically
  increases adaptivity. Further, our optimistic algorithms are guaranteed to do no
  worse than their non-optimistic counterparts regardless of the quality of the optimistic
  estimates provided to the algorithm.'
section: contributed
title: Combining Online Learning Guarantees
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cutkosky19b
month: 0
tex_title: Combining Online Learning Guarantees
firstpage: 895
lastpage: 913
page: 895-913
order: 895
cycles: false
bibtex_author: Cutkosky, Ashok
author:
- given: Ashok
  family: Cutkosky
date: 2019-06-25
address: 
publisher: PMLR
container-title: Proceedings of the Thirty-Second Conference on Learning Theory
volume: '99'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 6
  - 25
pdf: http://proceedings.mlr.press/v99/cutkosky19b/cutkosky19b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
